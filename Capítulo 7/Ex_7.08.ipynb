{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Exercise 7.8*\n",
    "\n",
    "#### Write the pseudocode for the o↵-policy action-value prediction algorithm described immediately above. Pay particular attention to the termination conditions for the recursion upon hitting the horizon or the end of episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em 7.13 é fácil ver que devido à esperança de $\\rho_t$ ser igual a 1, o segundo termo desaparece.\n",
    "\n",
    "Para (7.14), devemos ter que,\n",
    "\n",
    "$$\\gamma  [V_{h-1}(S_{t+1}) - \\rho_t Q_{h-1}(S_{t+1}, A_{t+1})] = 0$$\n",
    "\n",
    "que pode ser verificado por,\n",
    "\n",
    "$$\\sum_{a} \\pi (a|S_{t+1}) Q_{h-1}(S_{t+1}, A_{t+1}) - \\sum_{a} b(a|S_{t+1}) \\frac{\\pi (a|S_{t+1})}{b(a|S_{t+1})} Q_{h-1}(S_{t+1}, A_{t+1}) = 0$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
