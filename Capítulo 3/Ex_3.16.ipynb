{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Exercise 3.16:*\n",
    "\n",
    "**Now consider adding a constant c to all the rewards in an episodic task,\n",
    "such as maze running. Would this have any e↵ect, or would it leave the task unchanged\n",
    "as in the continuing task above? Why or why not? Give an example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Resposta 1:\n",
    "\n",
    "Para a equação (3.8) do livro, se adicionarmos uma constante c para cada recompensa, o valor final ficaria como mostrado na Equação (1).\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "    G_t' = \\sum_{k=0}^{T} (R_{t+k+1}+c).\n",
    "\\end{equation}\n",
    "\n",
    "Assim, pela propriedade distributiva da soma,\n",
    "\n",
    "\\begin{equation}\n",
    "    G_t' = \\sum_{k=0}^{T}  R_{t+k+1}+ \\sum_{k=0}^{T}  c.\n",
    "\\end{equation}\n",
    "\n",
    "Assim,\n",
    "\n",
    "\\begin{equation}\n",
    "    G_t' = G_t + cT,\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Sendo assim, o valor de $G_t$ muda a depender da política, pois caso o comprimento $T$ aumente, duas políticas inicialmente de mesmo retorno, passam a ter retorno distintos. Por exemplo, se tem uma política que visita 2 estados com recompensas $G^1_t = 0+3 = 3$ e outra política com $G^2_t = 0+0+0+3 = 3$, com adição de uma constante de valor $10$ para cada recompensa, os retornos passam a ser respectivamente, $G^1_t = 10+13=23$ e $G^2_t = 10+10+10+13 = 43$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
